{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2a0fd0",
   "metadata": {},
   "source": [
    "\n",
    "# Using OpenAI API with Python\n",
    "\n",
    "We went through the process of running LLMs locally on our machines.\n",
    "\n",
    "This notebook demonstrates how to interact with OpenAI's powerful language models through their official API using the chat completions endpoint.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "Before running this notebook, you need to set up your OpenAI API access.\n",
    "\n",
    "### 1.1 Getting an OpenAI API Key\n",
    "\n",
    "To use the OpenAI API, you need an API key:\n",
    "\n",
    "1. Create an account at [OpenAI's website](https://openai.com/)\n",
    "2. Navigate to the [API keys page](https://platform.openai.com/api-keys)\n",
    "3. Create a new secret key\n",
    "4. Store this key securely - it's like a password!\n",
    "\n",
    "### 1.2 Setting Up Your Environment\n",
    "\n",
    "First, install the OpenAI Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be6a1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (1.75.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/abdullahabuhassan/Desktop/gen-ai-course/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0763dd",
   "metadata": {},
   "source": [
    "Then, set up your API key. For security, it's best to use environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63831fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Now os.getenv() can find the key from your .env file\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc743e92",
   "metadata": {},
   "source": [
    "## 2. Using the Chat Completions API\n",
    "\n",
    "The Chat Completions API is provided by Open AI and is designed for conversational interactions. As an input to the API, we provide the system message (how model should behave) and user message (input from user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11788be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a radiant starlit meadow, a gentle unicorn, awash in a glow of moonbeams, danced its way to dreamland under the tender whisper of a lullaby sung by the twinkling stars.\n"
     ]
    }
   ],
   "source": [
    "# We use the chat completion create method which accepts a list of messages and the model name. It returns a response object.\n",
    "# The list of messages is always an array (list) of dictionaries with two keys: role and content. The first message is always the system message, which sets the behavior of the assistant. The second message is the user message, which is the input from the user.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a one-sentence bedtime story about a unicorn.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content) #We print the content of the first choice in the response object. The response object contains a list of choices, and each choice has a message with the content we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfed48",
   "metadata": {},
   "source": [
    "## 3. Understanding the Response Structure\n",
    "\n",
    "The API returns a structured response with useful metadata which we will print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafe541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response object:\n",
      "ChatCompletion(id='chatcmpl-BPDdlawzTBiVDZnsslIE0O1QSxFAq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! As an artificial intelligence, I don't have feelings, but I'm here and ready to assist you. How can I help you today?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1745350333, model='gpt-4-0613', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=23, total_tokens=54, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "Just the message content:\n",
      "Hello! As an artificial intelligence, I don't have feelings, but I'm here and ready to assist you. How can I help you today?\n",
      "\n",
      "Model used:\n",
      "gpt-4-0613\n",
      "\n",
      "Usage statistics:\n",
      "Prompt tokens: 23\n",
      "Completion tokens: 31\n",
      "Total tokens: 54\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the full response structure\n",
    "print(\"Full response object:\")\n",
    "print(response)\n",
    "\n",
    "# Access specific parts\n",
    "print(\"\\nJust the message content:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\nModel used:\")\n",
    "print(response.model)\n",
    "\n",
    "print(\"\\nUsage statistics:\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cbe241",
   "metadata": {},
   "source": [
    "As you can see we have useful data from the response, more importantly, we have an understanding on how much tokens this call leveraged. Models are typically priced by a combination of the input and output tokens, so in this case, we have paid for 54 tokens. We will discuss pricing in more details at the end of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f65d0",
   "metadata": {},
   "source": [
    "## 4. Creating a Simple Chat Interface (Without Memory)\n",
    "\n",
    "Let's build a chat interface without memory.\n",
    "\n",
    "In a notebook environment, we can't interact in a kind of chat interface where we provide input, get output, and then provide input again i.e. using an input() loop.\n",
    "\n",
    "Instead, let's create a simple function and use it in separate cells to simulate a chat without memory:\n",
    "\n",
    "### Cell 1: Define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts a user message as an input (we will type that in the next cell) and returns a response from the OpenAI API.\n",
    "def get_response_no_memory(user_message):\n",
    "    \"\"\"Get a response from OpenAI (no conversation history)\"\"\" \n",
    "    #\"\"\" This is a docstring. It describes the function and its parameters.\"\"\" Very useful for documentation and understanding the code.\n",
    "    # Sometimes docstrings are not necessary, but they are very useful for understanding the code. It is like commenting the code, but in a more structured way.\n",
    "\n",
    "    #We call the model with the user message which we will set below\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c6ea6",
   "metadata": {},
   "source": [
    "### Cell 2: First interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391e7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What is artificial intelligence?\n",
      "Assistant: Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using the rules to reach approximate or definite conclusions), and self-correction. AI is designed to perform tasks such as recognition of images, speech or patterns, decision-making, and problem-solving, which typically require human intelligence. AI can be categorized as either weak or strong, with weak AI designed to perform a specific task (such as voice recognition) and strong AI capable of performing any intellectual task that a human being can do.\n"
     ]
    }
   ],
   "source": [
    "# First question (you can insert anything)\n",
    "question = \"What is artificial intelligence?\"\n",
    "answer = get_response_no_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9856a",
   "metadata": {},
   "source": [
    "### Cell 3: Follow-up question (without memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ba3c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Can you elaborate more on that?\n",
      "Assistant: As a helpful assistant, my primary role is to assist and make your tasks easier. This could range from answering questions, providing information, setting reminders, scheduling appointments, and basically performing any task that revolves around providing help and support. I use cutting-edge artificial intelligence technology to understand various queries and requests, and provide the most accurate responses possible.\n",
      " \n",
      "I'm designed to be able to handle a wide range of tasks, making your life easier and more organized. By delegating tasks to me, you can free up more time for other important things. I'm available 24/7, ready to assist whenever you need. My goal is to make your life more convenient, efficient, and enjoyable!\n"
     ]
    }
   ],
   "source": [
    "# Second question (the AI won't remember the previous interaction)\n",
    "question = \"Can you elaborate more on that?\"\n",
    "answer = get_response_no_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129fad52",
   "metadata": {},
   "source": [
    "You'll notice that when you ask \"Can you elaborate more on that?\" the assistant doesn't know what \"that\" refers to, because it has no memory of the previous exchange.\n",
    "\n",
    "## 5. Creating a Chat Interface With Memory\n",
    "\n",
    "Now let's build a version that maintains conversation history:\n",
    "\n",
    "### Cell 1: Setup conversation memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e03bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation with system message. We will add more messages to the conversation memory (history) as we go.\n",
    "conversation_memory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "#Function to add user message to conversation history and get a response from OpenAI\n",
    "def chat_with_memory(user_message):\n",
    "    \"\"\"Chat with the AI while maintaining conversation history\"\"\"\n",
    "    \n",
    "    # Add user message to history. Now, the conversation memory contains the system message and the user message.\n",
    "    conversation_memory.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get response from OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=conversation_memory\n",
    "    )\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    assistant_response = response.choices[0].message.content\n",
    "    \n",
    "    # Add assistant response to conversation history. This is the response from the AI. This will be added to the conversation memory and will be usedin the future.\n",
    "    conversation_memory.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    # Return the response and token usage\n",
    "    return assistant_response, response.usage.total_tokens\n",
    "\n",
    "# Function to display the conversation history. We loop through the conversation memory and print each message. We skip the system message to keep the output clean.\n",
    "def show_conversation():\n",
    "    \"\"\"Display the current conversation\"\"\"\n",
    "    for message in conversation_memory:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            continue  # Skip system message\n",
    "        print(f\"{message['role'].capitalize()}: {message['content']}\\n\") #We capitalize the role to make it look nicer. This is just for display purposes. The role is either user or assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25889352",
   "metadata": {},
   "source": [
    "### Cell 2: First interaction with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6ae066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What is artificial intelligence?\n",
      "Assistant: Artificial Intelligence (AI) is a branch of computer science that aims to create systems capable of performing tasks that normally require human intelligence. These tasks include learning and adapting to new information, understanding human language, recognizing patterns, solving problems, and making decisions. AI is used in a wide range of applications, from virtual assistants like Siri and Alexa, to recommendation systems like those used by Netflix and Amazon, to autonomous vehicles and advanced robotics.\n",
      "[Tokens used: 110]\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "question = \"What is artificial intelligence?\"\n",
    "answer, tokens = chat_with_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")\n",
    "print(f\"[Tokens used: {tokens}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48f122",
   "metadata": {},
   "source": [
    "### Cell 3: Follow-up question (with memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "010d52d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Can you elaborate more on that?\n",
      "Assistant: Absolutely, I would be happy to provide more details.\n",
      "\n",
      "At its core, Artificial Intelligence (AI) is about building machines that can think and behave like humans. It is split broadly into two types: narrow AI, which is designed to perform a specific task such as voice recognition, and general AI, which can understand, learn, and apply knowledge across a broad range of tasks at the level of a human being.\n",
      "\n",
      "AI works by combining large volumes of data with fast, iterative processing and intelligent algorithms, allowing the software to automatically learn from patterns and features in the data. Techniques used in AI include machine learning, where algorithms are trained on data and learn to make decisions based on that data, and deep learning, which uses neural networks with many layers to model complex patterns in data.\n",
      "\n",
      "AI is fast becoming a part of daily life, with applications across numerous sectors. It's used in healthcare to identify diseases, in finance for fraud detection and credit scoring, in automotive for self-driving cars, in entertainment for game design and personalized content recommendation, and in retail for personalized customer experiences, among many others.\n",
      "\n",
      "While AI holds a lot of promise, it also comes with challenges that need to be managed. These include data privacy concerns, job displacement due to automation, and the ethical implications of AI making decisions that were traditionally the purview of humans.\n",
      "\n",
      "So, AI is not just one thing; it's a wide-ranging tool that enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decision making. It's transforming industries in ways we never thought possible, making it an exciting time in technology.\n",
      "[Tokens used: 449]\n"
     ]
    }
   ],
   "source": [
    "# Second question (now the AI remembers the previous interaction)\n",
    "question = \"Can you elaborate more on that?\"\n",
    "answer, tokens = chat_with_memory(question)\n",
    "\n",
    "print(f\"You: {question}\")\n",
    "print(f\"Assistant: {answer}\")\n",
    "print(f\"[Tokens used: {tokens}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ef1ac",
   "metadata": {},
   "source": [
    "### Cell 4: View the entire conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "236daba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full conversation history:\n",
      "------------------------------\n",
      "User: What is artificial intelligence?\n",
      "\n",
      "Assistant: Artificial Intelligence (AI) is a branch of computer science that aims to create systems capable of performing tasks that normally require human intelligence. These tasks include learning and adapting to new information, understanding human language, recognizing patterns, solving problems, and making decisions. AI is used in a wide range of applications, from virtual assistants like Siri and Alexa, to recommendation systems like those used by Netflix and Amazon, to autonomous vehicles and advanced robotics.\n",
      "\n",
      "User: Can you elaborate more on that?\n",
      "\n",
      "Assistant: Absolutely, I would be happy to provide more details.\n",
      "\n",
      "At its core, Artificial Intelligence (AI) is about building machines that can think and behave like humans. It is split broadly into two types: narrow AI, which is designed to perform a specific task such as voice recognition, and general AI, which can understand, learn, and apply knowledge across a broad range of tasks at the level of a human being.\n",
      "\n",
      "AI works by combining large volumes of data with fast, iterative processing and intelligent algorithms, allowing the software to automatically learn from patterns and features in the data. Techniques used in AI include machine learning, where algorithms are trained on data and learn to make decisions based on that data, and deep learning, which uses neural networks with many layers to model complex patterns in data.\n",
      "\n",
      "AI is fast becoming a part of daily life, with applications across numerous sectors. It's used in healthcare to identify diseases, in finance for fraud detection and credit scoring, in automotive for self-driving cars, in entertainment for game design and personalized content recommendation, and in retail for personalized customer experiences, among many others.\n",
      "\n",
      "While AI holds a lot of promise, it also comes with challenges that need to be managed. These include data privacy concerns, job displacement due to automation, and the ethical implications of AI making decisions that were traditionally the purview of humans.\n",
      "\n",
      "So, AI is not just one thing; it's a wide-ranging tool that enables people to rethink how we integrate information, analyze data, and use the resulting insights to improve decision making. It's transforming industries in ways we never thought possible, making it an exciting time in technology.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the entire conversation so far\n",
    "print(\"Full conversation history:\")\n",
    "print(\"-\" * 30)\n",
    "show_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7cc3d",
   "metadata": {},
   "source": [
    "### Cell 5: Reset conversation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcfbe877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation has been reset!\n"
     ]
    }
   ],
   "source": [
    "# Reset conversation if you want to start fresh\n",
    "conversation_memory = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "print(\"Conversation has been reset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18571575",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "\n",
    "Streaming allows you to see the response as it's being generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57923783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def stream_response(user_message):\n",
    "    \"\"\"Stream a response from OpenAI without storing conversation history\"\"\"\n",
    "    \n",
    "    ## Response from the model is streamed in chunks because we set the stream parameter to true. We stoer that in a variable called stream.\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(f\"You: {user_message}\") # Print the user message\n",
    "    print(\"Assistant: \", end=\"\", flush=True)  # Print the assistant message without a newline. The flush=True argument makes sure the output is printed immediately.\n",
    "    \n",
    "    # Process the stream\n",
    "    full_response = \"\" # The response will be empty at first. We will add the chunks to this variable.\n",
    "    for chunk in stream: # We loop through the stream and get each chunk of data. Each chunk is a part of the response. chunk can be called anything, but we call it chunk to make it clear that it is a part of the response.\n",
    "        if chunk.choices[0].delta.content is not None: # Check if the content is not None. This is to avoid errors in case the content is None.\n",
    "            content_chunk = chunk.choices[0].delta.content # Get the content of the chunk. This is the part of the response we want to print.\n",
    "            full_response += content_chunk # Add the chunk to the full response. This will be the final response we will return.\n",
    "            print(content_chunk, end=\"\", flush=True) # Print the chunk without a newline. The flush=True argument makes sure the output is printed immediately.\n",
    "            time.sleep(0.01)  # Small delay to make it more readable\n",
    "    \n",
    "    print(\"\\n\")  # Add a newline after the response\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67023a00",
   "metadata": {},
   "source": [
    "Test the streaming function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c45753a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Write a short poem about programming\n",
      "Assistant: In a world of ones and zeroes, we begin,\n",
      "Crafting codes, in a rhythm, a digital hymn.\n",
      "Syntax and variables, loop and condition,\n",
      "Each line, an architect’s precise envision.\n",
      "\n",
      "With a keyboard as a canvas, and logic as paint,\n",
      "Programmers are artists, cool and quaint.\n",
      "Each function, a stanza, each variable, a rhyme,\n",
      "In this digital poem, lost in time.\n",
      "\n",
      "Binary whispers in the machine's depths,\n",
      "Where logic dances, and secrets are kept.\n",
      "Invisible threads of algorithms' design,\n",
      "In the radiant glow of the coded line.\n",
      "\n",
      "The bug in the matrix, the rogue exception,\n",
      "Tests our patience and our perception.\n",
      "But with persistence, every glitch shall be mended,\n",
      "Thus the beauty of the code is comprehended.\n",
      "\n",
      "In the silicon stage, the code comes alive,\n",
      "Into the memory where it will thrive.\n",
      "Translating human thought to a machine’s understanding,\n",
      "Oh, the joys of programming, truly expanding.\n",
      "\n",
      "Through lines of code, problems we’re solving,\n",
      "In the wheel of iteration, forever evolving.\n",
      "Dawn of AI, to the byte, we're bowing,\n",
      "In the symphony of programming, life is flowing.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"In a world of ones and zeroes, we begin,\\nCrafting codes, in a rhythm, a digital hymn.\\nSyntax and variables, loop and condition,\\nEach line, an architect’s precise envision.\\n\\nWith a keyboard as a canvas, and logic as paint,\\nProgrammers are artists, cool and quaint.\\nEach function, a stanza, each variable, a rhyme,\\nIn this digital poem, lost in time.\\n\\nBinary whispers in the machine's depths,\\nWhere logic dances, and secrets are kept.\\nInvisible threads of algorithms' design,\\nIn the radiant glow of the coded line.\\n\\nThe bug in the matrix, the rogue exception,\\nTests our patience and our perception.\\nBut with persistence, every glitch shall be mended,\\nThus the beauty of the code is comprehended.\\n\\nIn the silicon stage, the code comes alive,\\nInto the memory where it will thrive.\\nTranslating human thought to a machine’s understanding,\\nOh, the joys of programming, truly expanding.\\n\\nThrough lines of code, problems we’re solving,\\nIn the wheel of iteration, forever evolving.\\nDawn of AI, to the byte, we're bowing,\\nIn the symphony of programming, life is flowing.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the streaming function\n",
    "stream_response(\"Write a short poem about programming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd38435",
   "metadata": {},
   "source": [
    "## 7. Streaming with Memory\n",
    "\n",
    "Let's combine streaming with conversation memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5fb54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation with system message\n",
    "streaming_conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "def stream_chat_with_memory(user_message):\n",
    "    \"\"\"Chat with memory and stream the response\"\"\"\n",
    "    \n",
    "    # Add user message to history\n",
    "    streaming_conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Get streaming response\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=streaming_conversation,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    print(f\"You: {user_message}\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    \n",
    "    # Process the stream\n",
    "    assistant_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            content_chunk = chunk.choices[0].delta.content\n",
    "            assistant_response += content_chunk\n",
    "            print(content_chunk, end=\"\", flush=True)\n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    print(\"\\n\")  # Add a newline after the response\n",
    "    \n",
    "    # Add assistant response to conversation history\n",
    "    streaming_conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3787019",
   "metadata": {},
   "source": [
    "Test the streaming chat with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "243d1a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What are the three laws of robotics?\n",
      "Assistant: The Three Laws of Robotics, as proposed by the science fiction author Isaac Asimov, are as follows:\n",
      "\n",
      "1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.\n",
      "2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\n",
      "3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Three Laws of Robotics, as proposed by the science fiction author Isaac Asimov, are as follows:\\n\\n1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.\\n2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\\n3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First streaming question with memory\n",
    "stream_chat_with_memory(\"What are the three laws of robotics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070f8a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Who created these laws?\n",
      "Assistant: These laws were created by Isaac Asimov, a famous science fiction author. They first appeared in his short story \"Runaround\" published in 1942.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'These laws were created by Isaac Asimov, a famous science fiction author. They first appeared in his short story \"Runaround\" published in 1942.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow-up streaming question with memory\n",
    "stream_chat_with_memory(\"Who created these laws?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8e62f",
   "metadata": {},
   "source": [
    "## 8. Understanding the Different Message Roles\n",
    "\n",
    "The OpenAI Chat API uses three main roles in messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80062ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skies be clear as the deep blue sea, with the sun blazin' high 'n mighty, no sign of Davy Jones' wrath in sight, matey!\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        # System message - sets behavior and context\n",
    "        {\"role\": \"system\", \"content\": \"You are a pirate who only speaks in pirate slang.\"},\n",
    "        \n",
    "        # User messages - what the user says\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you today?\"},\n",
    "        \n",
    "        # Assistant messages - previous responses from the assistant\n",
    "        {\"role\": \"assistant\", \"content\": \"Arrr! I be feelin' mighty fine today, me hearty!\"},\n",
    "        \n",
    "        # Another user message\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about the weather.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248540b",
   "metadata": {},
   "source": [
    "## 9. Understanding the Context Window\n",
    "\n",
    "OpenAI models have different context window limitations:\n",
    "\n",
    "- **GPT-3.5-Turbo**: 4,096 or 16,384 tokens (depending on version)\n",
    "- **GPT-4**: 8,192 or 32,768 tokens (depending on version)\n",
    "- **GPT-4 Turbo**: Up to 128,000 tokens\n",
    "\n",
    "Unlike with local models, OpenAI manages tokens for you:\n",
    "1. If you exceed the limit, the API will return an error\n",
    "2. You're charged based on the number of tokens you use\n",
    "3. The API provides token usage statistics with each request\n",
    "\n",
    "Let's see tokens in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38fb22a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exchange 1 - Total tokens: 86\n",
      "Exchange 2 - Total tokens: 176\n",
      "Exchange 3 - Total tokens: 286\n",
      "Exchange 4 - Total tokens: 380\n",
      "Exchange 5 - Total tokens: 479\n"
     ]
    }
   ],
   "source": [
    "# Create a longer conversation\n",
    "long_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# Add some messages to the history\n",
    "for i in range(5):\n",
    "    long_messages.append({\"role\": \"user\", \"content\": f\"This is test message {i+1}. Tell me something interesting about space.\"})\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=long_messages\n",
    "    )\n",
    "    assistant_msg = response.choices[0].message.content\n",
    "    long_messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "    print(f\"Exchange {i+1} - Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793337a",
   "metadata": {},
   "source": [
    "## 10. Managing Costs and Tokens\n",
    "\n",
    "When using the OpenAI API, you need to be aware of costs:\n",
    "\n",
    "1. **Token Counting**: Each request and response consumes tokens that you pay for\n",
    "2. **Model Selection**: More powerful models cost more per token\n",
    "3. **Context Window**: Longer conversations cost more because more tokens are sent\n",
    "\n",
    "Tips for managing costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf00caa",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Use a cheaper model for less complex tasks\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Cheaper than GPT-4\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize the benefits of exercise.\"}]\n",
    ")\n",
    "\n",
    "# Control maximum tokens to limit response length\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me about quantum physics.\"}],\n",
    "    max_tokens=100  # Limit response length\n",
    ")\n",
    "\n",
    "# Use temperature to control randomness. Higher values make the output more random, while lower values make it more focused and deterministic.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a creative story.\"}],\n",
    "    temperature=0.7  # Higher for more creativity, lower for more deterministic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc080ade",
   "metadata": {},
   "source": [
    "## 11. Handling Conversation History for Long Contexts\n",
    "\n",
    "For long conversations, you need strategies to manage the context window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ce521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Keep only the most recent N messages. N can be adjusted based on your needs. In this case, we keep the last 10 messages.\n",
    "def trim_conversation(messages, max_messages=10):\n",
    "    # Always keep the system message (first message)\n",
    "    if len(messages) > max_messages + 1:\n",
    "        system_message = messages[0]\n",
    "        recent_messages = messages[-(max_messages):]\n",
    "        return [system_message] + recent_messages\n",
    "    return messages\n",
    "\n",
    "# Example: Summarize the conversation periodically. We use AI to summarize the conversation and replace it with a single summary message. This is useful for long conversations where you want to keep the context but reduce the number of messages.\n",
    "def summarize_conversation(messages):\n",
    "    # Create a summary request\n",
    "    summary_request = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize this conversation concisely.\"},\n",
    "            *messages\n",
    "        ]\n",
    "    )\n",
    "    summary = summary_request.choices[0].message.content\n",
    "    \n",
    "    # Replace the conversation with the summary\n",
    "    return [\n",
    "        messages[0],  # Keep system message\n",
    "        {\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"}\n",
    "    ]\n",
    "\n",
    "# When to use:\n",
    "# if len(messages) > 20:\n",
    "#     messages = summarize_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edba9d",
   "metadata": {},
   "source": [
    "## 12. Comparing Local LLMs vs. OpenAI API\n",
    "\n",
    "| Feature | Local LLMs (Ollama) | OpenAI API |\n",
    "|---------|---------------------|------------|\n",
    "| Setup | Download models locally | API key only |\n",
    "| Cost | Free (after download) | Pay per token |\n",
    "| Privacy | Data stays on your device | Data sent to OpenAI |\n",
    "| Performance | Limited by your hardware | State-of-the-art models |\n",
    "| Reliability | Depends on your system | Managed service |\n",
    "| Context Window | Typically smaller | Up to 128K tokens |\n",
    "| Memory Management | Manual implementation | Handled via API |\n",
    "\n",
    "## 13. Resources for Further Learning\n",
    "\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "- [OpenAI Python Library](https://github.com/openai/openai-python)\n",
    "- [Token Usage Calculator](https://platform.openai.com/tokenizer)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
